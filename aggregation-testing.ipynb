{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43dcb3e",
   "metadata": {},
   "source": [
    "# Aggregation testing\n",
    "\n",
    "Let's see how TSAM works.\n",
    "The first challenge is naturally just getting NE-model data read into TSAM.\n",
    "Probably easiest to do this from the .gdx files directly, as the raw NE-data is\n",
    "annoyingly varied in its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea9fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary packages\n",
    "\n",
    "import gams.transfer as gt # Read .gdx data\n",
    "import tsam.timeseriesaggregation as tsam # Timeseries aggregation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e798ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define stuff for testing\n",
    "# NOTE! You will likely have to tweak these to get things working\n",
    "# depending on how you've installed the NE-model.\n",
    "\n",
    "scenario_name = \"National_Trends_2040_nucTypical\"\n",
    "year = 1982\n",
    "input_folder_path = f\"./north_european_model/input_{scenario_name}/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read timeseries and extract DataFrame for testing\n",
    "\n",
    "gdx = gt.Container(input_folder_path + f\"ts_cf_PV_{year}.gdx\")\n",
    "df = gdx[\"ts_cf\"].records\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pivot data for TSAM\n",
    "\n",
    "df = df.pivot(\n",
    "    index=[\"flow\", \"f\", \"t\"],\n",
    "    columns=\"node\",\n",
    "    values=\"value\"\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate using TSAM?\n",
    "\n",
    "aggregation = tsam.TimeSeriesAggregation(\n",
    "    df,\n",
    "    noTypicalPeriods=4,\n",
    "    hoursPerPeriod=24,\n",
    "    clusterMethod=\"hierarchical\",\n",
    "    resolution=1,\n",
    "    rescaleClusterPeriods=False, # This disables automatic rescaling of cluster data.\n",
    "    segmentation=True, # This is required for hypertuning, but messes up the comparison.\n",
    ")\n",
    "typical_periods = aggregation.createTypicalPeriods()\n",
    "aggregation.clusterCenterIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0796ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check cluster order.\n",
    "\n",
    "aggregation.clusterOrder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eab075",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot comparison?\n",
    "\n",
    "typical_periods.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b83064",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot raw data for the corresponding inds.\n",
    "\n",
    "cluster_inds = aggregation.clusterCenterIndices\n",
    "hour_inds = [val * aggregation.hoursPerPeriod + i for val in cluster_inds for i in range(1,aggregation.hoursPerPeriod)]\n",
    "df2 = df.iloc[hour_inds]\n",
    "df2.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290a62ce",
   "metadata": {},
   "source": [
    "## Seems to work surprisingly easy?\n",
    "\n",
    "Overall, using TSAM is surprisingly easy.\n",
    "Should be more than doable to run this clustering for the entire NE-model, although I have no guarantee that it is going to be computationally feasible.\n",
    "The `k-medoids` is much more computationally intensive than e.g. the `hierarchical` clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be767d33",
   "metadata": {},
   "source": [
    "## What about hypertuning?\n",
    "\n",
    "This supposedly finds better segment numbers and durations for the data.\n",
    "However, seems to require \"Segmentation\", which I'm not sure what it means.\n",
    "\n",
    "WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d94b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test aggregation hypertuning\n",
    "# Disabled for now, as this takes forever.\n",
    "\n",
    "import tsam.hyperparametertuning as hype\n",
    "hyper = hype.HyperTunedAggregations(aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248cfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS TAKES A LONG TIME! ~1 HOUR!\n",
    "# Don't enable unless you really mean it!\n",
    "\n",
    "#hyper.identifyParetoOptimalAggregation()\n",
    "\n",
    "# Seems to run over all possible aggregations or something,\n",
    "# no wonder it takes a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimal segment period combination seems more promising,\n",
    "# as it doesn't take ages to run.\n",
    "\n",
    "reduction_factor = 4 * 168 / 8760 # Determine how much we want to reduce the data.\n",
    "hyper.identifyOptimalSegmentPeriodCombination(reduction_factor)\n",
    "\n",
    "# So apparently 56 12-hour periods seems to be the optimal, not 4 consecutive weeks?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
