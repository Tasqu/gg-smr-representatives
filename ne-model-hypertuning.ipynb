{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba985640",
   "metadata": {},
   "source": [
    "# NE-model hypertuning\n",
    "\n",
    "This notebook provides a way to test hypertuning of NE-model data across all its weather years.\n",
    "\n",
    ">**NOTE!**\n",
    ">You need to have `tsam_df_dict.pkl` available for this script to run.\n",
    ">This file is produced in `representative-period-processing.ipynb` after formatting raw data for TSAM.\n",
    "\n",
    "This script consists of the following main steps:\n",
    "1. Python setup and loading preprocessed input data.\n",
    "2. TSAM aggregation based on user settings.\n",
    "3. TSAM hypertuning using the aggregation from the previous step as the basis.\n",
    "4. Hypertuning diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a05fc5d",
   "metadata": {},
   "source": [
    "## 1. Setup and input data\n",
    "\n",
    "Imports the necessary packages,\n",
    "suppresses TSAM warnings to avoid excessive printing,\n",
    "and loads the preprocessed data from `representative-period-processing.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e6f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary packages\n",
    "\n",
    "import pickle # Load preprocessed input data to save time.\n",
    "import os # Check if results exist.\n",
    "import tsam.timeseriesaggregation as tsam # Timeseries aggregation.\n",
    "from multiprocessing import cpu_count # Count CPUs\n",
    "import pickle # Save hypertuning results to save time.\n",
    "# Suppress warnings from TSAM\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b0cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load input data\n",
    "\n",
    "with open(\"tsam_df_dict.pkl\", \"rb\") as file:\n",
    "    tsam_df_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43499b55",
   "metadata": {},
   "source": [
    "## 2. TSAM aggregation\n",
    "\n",
    "Provides settings for TSAM aggregation to play with,\n",
    "and then proceeds to do the aggregation.\n",
    "\n",
    ">**NOTE!**\n",
    ">The TSAM aggregation settings are used as the basis for the hypertuning.\n",
    ">Furthermore, the given `noTypicalPeriods * hoursPerPeriod / <full_year_hours>` is used to calculate\n",
    ">the desired `reduction_factor` for the hypertuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335fac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configure TSAM aggregation\n",
    "# Copied from `representative-period-processing.ipynb`\n",
    "# (see https://tsam.readthedocs.io/en/latest/timeseriesaggregationDoc.html)\n",
    "\n",
    "# Main settings you might be interested in tweaking.\n",
    "noTypicalPeriods = 4 # Number of representative periods. (Periods for hypertuning)\n",
    "hoursPerPeriod = 168 # Hours per representative period. (Segments for hypertuning?)\n",
    "extremePeriodMethod = \"replace_cluster_center\" # Method to integrate extreme periods?\n",
    "clusterMethod = \"hierarchical\" # Select clustering method. (`hierarchical` or `k_medoids` recommended)\n",
    "\n",
    "# Calculate a hash based on the main settings to archive results.\n",
    "settings_hash = f\"{noTypicalPeriods}x{hoursPerPeriod}h-{extremePeriodMethod}-{clusterMethod}\"\n",
    "\n",
    "# Auxiliary settings you shoudldn't need to touch.\n",
    "resolution = 1 # Resolution of input data in hours, shouldn't need to be touched.\n",
    "rescaleClusterPeriods = False # Don't rescale periods, we don't use that data anyhow.\n",
    "segmentation = True # Segmentation to enable hypertuning, doesn't impact the clustering results.\n",
    "numericalTolerance = 1e-6 # Set numerical tolerance for TSAM so it stops complaining (doesn't seem to work for some reason)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c05081",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TSAM time series aggregation\n",
    "# Copied from `representative-period-processing.ipynb`\n",
    "\n",
    "tsam_dict = dict() # Initialize dict to store TSAM aggregation per year.\n",
    "for (year, data) in tsam_df_dict.items():\n",
    "    aggregation = tsam.TimeSeriesAggregation( ## Define TSAM aggregation.\n",
    "        data,\n",
    "        noTypicalPeriods=noTypicalPeriods,\n",
    "        hoursPerPeriod=hoursPerPeriod,\n",
    "        clusterMethod=clusterMethod,\n",
    "        resolution=resolution,\n",
    "        rescaleClusterPeriods=rescaleClusterPeriods,\n",
    "        extremePeriodMethod=extremePeriodMethod,\n",
    "        segmentation=segmentation,\n",
    "        numericalTolerance=numericalTolerance,\n",
    "    )\n",
    "    aggregation.createTypicalPeriods() ## Run TSAM aggregation.\n",
    "    tsam_dict[year] = aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c52f2",
   "metadata": {},
   "source": [
    "## 3. TSAM hypertuning\n",
    "\n",
    "Parallel TSAM hypertuning over the full year set.\n",
    "In order to save time, hypertuning results are saved after their `settings_hash`,\n",
    "calculated based on the main settings for the TSAM aggregation in the previous section.\n",
    "The user can force redo hypertuning using the settings below,\n",
    "in addition to manually tweaking the `num_workers` and `reduction_factor`.\n",
    "\n",
    ">**NOTE!**\n",
    ">The hypertuning will likely take tens of minutes of not hours to run over the entire dataset!\n",
    ">Caution is advised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hypertuning settings\n",
    "\n",
    "# Option to force hypertune.\n",
    "force_hypertune = False\n",
    "hypertune = (force_hypertune or not os.path.exists(f\"{settings_hash}.pkl\"))\n",
    "\n",
    "# Set number of processors for multiprocessing. (CPU count - 1 default)\n",
    "num_workers = cpu_count() - 1\n",
    "\n",
    "# Data reduction factor calculated based on given aggregation settings.\n",
    "reduction_factor = (\n",
    "    noTypicalPeriods * hoursPerPeriod / len(next(iter(tsam_df_dict.values())))\n",
    ")\n",
    "(hypertune, num_workers, reduction_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hypertuning across all years.\n",
    "# NOTE! This is likely going to take tens of minutes!\n",
    "# Seems to be around ~15-20 min, but this might depend on aggregation settings?\n",
    "# For some reason, Jupyter notebooks don't work with multiprocessing directly.\n",
    "\n",
    "if hypertune:\n",
    "    from multihyper import parallel_hypertuning\n",
    "    hypertuned = parallel_hypertuning(tsam_dict, reduction_factor, num_workers)\n",
    "    with open(f\"{settings_hash}.pkl\", \"wb\") as file: # Save results for future use.\n",
    "        pickle.dump(hypertuned, file)\n",
    "else:\n",
    "    with open(f\"{settings_hash}.pkl\", \"rb\") as file: # Load previous results.\n",
    "        hypertuned = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ffd690",
   "metadata": {},
   "source": [
    "## 4. Hypertuning diagnostics\n",
    "\n",
    "Examine the results of hypertuning.\n",
    "We're mainly interested in how TSAM thinks each year should be represented,\n",
    "and if this changes from year to year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate original TSAM aggregation accuracy indicators for comparison\n",
    "# Parallelized to save time, otherwise takes surprisingly long.\n",
    "\n",
    "from multihyper import parallel_diagnostics\n",
    "tsam_diag = parallel_diagnostics(tsam_dict, num_workers)\n",
    "\n",
    "# Overall, it seems that TSAM generally does better with a lot of short periods,\n",
    "# instead of a limited number of longer periods.\n",
    "# At least with all the NE-model data formatted as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine results, first for the desired aggregation.\n",
    "\n",
    "tsam_diag[\"1990\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Then for the hypertuned one\n",
    "\n",
    "hypertuned[\"1990\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd72e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check counts for different aggregations\n",
    "# This actually requires hypertuning to have been run.\n",
    "\n",
    "segments_and_periods = [ # Extract segment and period information from hypertuning results.\n",
    "    (seg, per) for (year, (seg, per, diag)) in hypertuned.items()\n",
    "]\n",
    "hypertuned_counts = { # Count unique segment-period pairs.\n",
    "    seg_per: segments_and_periods.count(seg_per)\n",
    "    for seg_per in set(segments_and_periods)\n",
    "}\n",
    "hypertuned_counts # Reported in hours x periods\n",
    "\n",
    "# Updated to contain all timeseries.\n",
    "# Seems like with too many timeseries, hypertuning is more or less useless?\n",
    "# Or maybe it has something to do with the extreme period method?\n",
    "# 4x24h -> 96x1h?\n",
    "# 8x24h -> 192x1h? \n",
    "# 12x24h -> 288x1h?\n",
    "# 4x168h -> 51x13h\n",
    "# 5x168h -> Crashes due to insufficient memory...\n",
    "# 8x168h -> Crashes due to insufficient memory..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
