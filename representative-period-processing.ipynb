{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9efe7e",
   "metadata": {},
   "source": [
    "# Representative period processing\n",
    "\n",
    "This script processes representative periods for the NE-model.\n",
    "The processing consists of the following steps:\n",
    "1. Python configuration for the script.\n",
    "2. Read `.gdx` time series from a pre-processed NE-model input folder.\n",
    "3. Format them into a single massive dataframe for TSAM.\n",
    "4. Run TSAM and process the representatives into Backbone samples.\n",
    "5. Output Backbone settings.\n",
    "\n",
    "You will need to have an input folder processed by the NE-model `building_input_data.py`,\n",
    "as this scrip will use the preprocessed `.gdx` time series files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75742990",
   "metadata": {},
   "source": [
    "## 1. Config\n",
    "\n",
    "Import the necessary packages etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary packages\n",
    "\n",
    "import os # For gathering all timeseries files.\n",
    "from itertools import product # More efficient looping\n",
    "import gams.transfer as gt # For reading said timeseries files.\n",
    "import tsam.timeseriesaggregation as tsam # For aggregating representatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6816efa6",
   "metadata": {},
   "source": [
    "## 2. Read time series input data\n",
    "\n",
    "Here, we read the time series input data from the given `input_folder_path`.\n",
    "The `omit_suffixes`, `aggregate_weather_years` and `aggregate_timeseries_names` filters are also applied\n",
    "here to avoid unnecessary input data reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configure input data to be included for aggregation.\n",
    "\n",
    "# Path to the NE-model preprocessed input data folder.\n",
    "# The one containing a ton of ts_*.gdx files)\n",
    "input_folder_path = \"./north_european_model/input_National_Trends_2040_nucTypical/\"\n",
    "\n",
    "# Select weather years to be processed `list(<str>)`!!!\n",
    "# `None` processes all available years.\n",
    "aggregate_weather_years = None\n",
    "\n",
    "# Select time series names for aggregation `list(<str>)`.\n",
    "# `None` processes all available timeseries.\n",
    "aggregate_timeseries_names = None\n",
    "\n",
    "# Omit `.gdx` files with these as their suffix.\n",
    "omit_suffixes = [\"forecasts\", \"demands\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gather and filter the timeseries files in the input folder.\n",
    "\n",
    "# Reading and preprocessing relevant filenames.\n",
    "timeseries_files = os.listdir(input_folder_path) # Gather all files in the dir\n",
    "timeseries_files = [f.split('.') for f in timeseries_files] # Split file suffix\n",
    "timeseries_files = [f[0].split('_') for f in timeseries_files if f[-1] == \"gdx\"] # Split filename by '_' and filter by .gdx\n",
    "timeseries_files = [('_'.join(f[0:-1]), f[-1]) for f in timeseries_files if f[0] == \"ts\"] # Form filenames/years and filter by 'ts' prefix\n",
    "\n",
    "# Config filtering by suffix, timeseries names, and weather years.\n",
    "if omit_suffixes is not None:\n",
    "    timeseries_files = [f for f in timeseries_files if f[-1] not in omit_suffixes]\n",
    "if aggregate_weather_years is not None:\n",
    "    timeseries_files = [f for f in timeseries_files if f[-1] in aggregate_weather_years]\n",
    "if aggregate_timeseries_names is not None:\n",
    "    timeseries_files = [f for f in timeseries_files if f[0] in aggregate_timeseries_names]\n",
    "\n",
    "# Determine sets of remaining filenames and years.\n",
    "filtered_filenames = set([f[0] for f in timeseries_files])\n",
    "filtered_years = set([f[-1] for f in timeseries_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea58b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read .gdx data into a nested dictionary with `year`->`param_name` as the keys.\n",
    "\n",
    "gdx_df_dict = dict() # Initialize empty dict for collecting input data.\n",
    "for (year, filename) in product(filtered_years, filtered_filenames):\n",
    "    gdx = gt.Container(f\"{input_folder_path}{filename}_{year}.gdx\") # Read input data file.\n",
    "    gdx_df_dict.setdefault(year, dict()) # Initialize empty sub-dictionary for parameter values.\n",
    "    for (param_name, vals) in gdx.data.items():\n",
    "        gdx_df_dict[year][param_name] = vals.records # Record values per year per param_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919d7cd",
   "metadata": {},
   "source": [
    "## 3. Format data for TSAM\n",
    "\n",
    "All the data needs to be in a single dataframe with timesteps as indices for TSAM,\n",
    "while all timeseries values need to be pivoted to columns.\n",
    "\n",
    ">**NOTE!**\n",
    ">Currently, all timeseries are used raw, as-is, without any weighting.\n",
    ">Better results might be achieved via some form of weighting, but I'm unsure how that should be implemented in TSAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a1ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings for TSAM data formatting\n",
    "# Don't change these unless you know what you're doing.\n",
    "\n",
    "index_column_name = 't' # Name of the time index column.\n",
    "value_column_name = 'value' # Name of the value column.\n",
    "forecast_filter = {'f00'} # Filter out forecasts besides these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca2ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format data for TSAM\n",
    "# This unfortunately seems to take ~1.5 min for the full dataset.\n",
    "\n",
    "tsam_df_dict = dict() # Initialize empty dict for collecting all timeseries per year.\n",
    "for (year, param_dict) in gdx_df_dict.items():\n",
    "    for (param_name, vals) in param_dict.items():\n",
    "        agg_cols = vals.columns.difference([index_column_name, value_column_name]) # Gather all other column names to be aggregated.\n",
    "        agg_col_name = '-'.join([param_name, *agg_cols]) # Name for the aggregated column including parameter name.\n",
    "        if vals.get('f') is not None: # Forecast filtering applied when needed.\n",
    "            vals = vals.loc[vals['f'].isin(forecast_filter)]\n",
    "        vals[agg_col_name] = param_name + '-' + vals[agg_cols].agg('-'.join, axis=1) # Create aggregate column value.\n",
    "        vals = vals[[index_column_name, value_column_name, agg_col_name]] # Omit old columns.\n",
    "        vals = vals.pivot( # Pivot timeseries dataframe for TSAM\n",
    "            index=index_column_name, columns=agg_col_name, values=value_column_name\n",
    "        )\n",
    "        if tsam_df_dict.get(year) is None:\n",
    "            tsam_df_dict[year] = vals # First dataframe becomes the starting point.\n",
    "        else:\n",
    "            tsam_df_dict[year] = tsam_df_dict[year].join(vals) # Rest are joined on index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13449e27",
   "metadata": {},
   "source": [
    "## 4. Run TSAM and process Backbone samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configure TSAM aggregation\n",
    "# (see https://tsam.readthedocs.io/en/latest/timeseriesaggregationDoc.html)\n",
    "\n",
    "noTypicalPeriods = 4 # Number of representative periods.\n",
    "hoursPerPeriod = 168 # Hours per representative period.\n",
    "resolution = 1 # Resolution of input data in hours.\n",
    "clusterMethod = \"hierarchical\" # Select clustering method. (`hierarchical` or `k_medoids` recommended)\n",
    "rescaleClusterPeriods = False # Don't rescale periods, we don't use that data anyhow.\n",
    "extremePeriodMethod = 'None' # Method to integrate extreme periods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ca8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TSAM time series aggregation\n",
    "# This seems amazingly fast, less than 5 seconds for all the data.\n",
    "\n",
    "tsam_dict = dict() # Initialize dict to store TSAM aggregation per year.\n",
    "for (year, data) in tsam_df_dict.items():\n",
    "    aggregation = tsam.TimeSeriesAggregation( ## Define TSAM aggregation.\n",
    "        data,\n",
    "        noTypicalPeriods=noTypicalPeriods,\n",
    "        hoursPerPeriod=hoursPerPeriod,\n",
    "        clusterMethod=clusterMethod,\n",
    "        resolution=resolution,\n",
    "        rescaleClusterPeriods=rescaleClusterPeriods,\n",
    "        extremePeriodMethod=extremePeriodMethod,\n",
    "    )\n",
    "    aggregation.createTypicalPeriods() ## Run TSAM aggregation.\n",
    "    tsam_dict[year] = aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b172b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process Backbone samples per year from the aggregation\n",
    "# The samples are processed based on the \"clusters\" in TSAM.\n",
    "# NOTE! There is no guarantee that TSAM samples occur in a sequence similar to Backbone samples!\n",
    "# However, based on very brief testing this seems to be the case? \n",
    "\n",
    "sample_dict = dict() # Initialize dict to store Backbone samples per year.\n",
    "for (year, agg) in tsam_dict.items():\n",
    "    # First timestep index of each sample, directly from cluster center indices.\n",
    "    sample_start_indices = agg.clusterCenterIndices\n",
    "    # Calculate sample weights based on the cluster occurrence number.\n",
    "    sample_weights = agg.clusterPeriodNoOccur\n",
    "    sample_weights = {\n",
    "        ind: val/sum(sample_weights.values()) for (ind, val) in sample_weights.items()\n",
    "    }\n",
    "    # Figure out the order the samples occur during the year\n",
    "    sample_order = agg.clusterOrder # Order of samples representing the raw data (year).\n",
    "    # TODO\n",
    "    # How do we actually want to represent the year?\n",
    "    # If we use the periods where they actually occur (clusterCenterIndices),\n",
    "    # the weights won't match the outcome desired by TSAM when fed to Backbone.\n",
    "    # If we instead match the weights, the periods cannot occur where Backbone needs them to.\n",
    "    # Then again, we need to place the samples where they are to get the correct data\n",
    "    # into Backbone (agg.clusterCenterIndices), so I guess we can't really consider the order?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e062532d",
   "metadata": {},
   "source": [
    "# TODO?\n",
    "\n",
    "Interesting idea by Niina: First reduce the number of years by selecting \"representative years\", then represent them using \"representative periods\".\n",
    "This could significantly reduce the computational burden to represent multiple years.\n",
    "However, we would likely want to include different weather years for the investments, so I'm not sure if TSAM can handle that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217bb786",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
