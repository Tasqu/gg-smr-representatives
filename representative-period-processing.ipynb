{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9efe7e",
   "metadata": {},
   "source": [
    "# Representative period processing\n",
    "\n",
    "This script processes representative periods for the NE-model.\n",
    "The processing consists of the following steps:\n",
    "1. Python configuration for the script.\n",
    "2. Read `.gdx` time series from a pre-processed NE-model input folder.\n",
    "3. Format them into a single massive dataframe for TSAM.\n",
    "4. Run TSAM and process the representatives into Backbone samples.\n",
    "5. Output Backbone settings.\n",
    "\n",
    "You will need to have an input folder processed by the NE-model `building_input_data.py`,\n",
    "as this scrip will use the preprocessed `.gdx` time series files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75742990",
   "metadata": {},
   "source": [
    "## 1. Config\n",
    "\n",
    "Import the necessary packages etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary packages\n",
    "\n",
    "import os # For gathering all timeseries files.\n",
    "from itertools import product, zip_longest # More efficient looping\n",
    "import gams.transfer as gt # For reading said timeseries files.\n",
    "import pickle # Save TSAM formatted data for reuse.\n",
    "import tsam.timeseriesaggregation as tsam # For aggregating representatives.\n",
    "import tsam.hyperparametertuning as hype # For testing optimizing representatives.\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a6f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Force data processing rerun if desired.\n",
    "\n",
    "force_rerun_data = False # Define whether to force data processing rerun. Set this to `True` if you're uncertain.\n",
    "rerun_data = (force_rerun_data or not os.path.exists(\"tsam_df_dict.pkl\")) # Rerun data if forced or no previous data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6816efa6",
   "metadata": {},
   "source": [
    "## 2. Read time series input data\n",
    "\n",
    "Here, we read the time series input data from the given `input_folder_path`.\n",
    "The `omit_suffixes`, `aggregate_weather_years` and `aggregate_timeseries_names` filters are also applied\n",
    "here to avoid unnecessary input data reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configure input data to be included for aggregation.\n",
    "\n",
    "# Path to the NE-model preprocessed input data folder.\n",
    "# The one containing a ton of ts_*.gdx files)\n",
    "input_folder_path = \"./north_european_model/input_National_Trends_2040_nucTypical/\"\n",
    "\n",
    "# Select weather years to be processed `list(<str>)`!!!\n",
    "# `None` processes all available years.\n",
    "aggregate_weather_years = None\n",
    "\n",
    "# Select time series names for aggregation `list(<str>)`.\n",
    "# `None` processes all available timeseries.\n",
    "aggregate_timeseries_names = None\n",
    "\n",
    "# Omit `.gdx` files with these as their suffix.\n",
    "omit_suffixes = [\"forecasts\", \"demands\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gather and filter the timeseries files in the input folder.\n",
    "\n",
    "# Reading and preprocessing relevant filenames.\n",
    "timeseries_files = os.listdir(input_folder_path) # Gather all files in the dir\n",
    "timeseries_files = [f.split('.') for f in timeseries_files] # Split file suffix\n",
    "timeseries_files = [f[0].split('_') for f in timeseries_files if f[-1] == \"gdx\"] # Split filename by '_' and filter by .gdx\n",
    "timeseries_files = [('_'.join(f[0:-1]), f[-1]) for f in timeseries_files if f[0] == \"ts\"] # Form filenames/years and filter by 'ts' prefix\n",
    "\n",
    "# Config filtering by suffix, timeseries names, and weather years.\n",
    "if omit_suffixes is not None:\n",
    "    timeseries_files = [f for f in timeseries_files if f[-1] not in omit_suffixes]\n",
    "if aggregate_weather_years is not None:\n",
    "    timeseries_files = [f for f in timeseries_files if f[-1] in aggregate_weather_years]\n",
    "if aggregate_timeseries_names is not None:\n",
    "    timeseries_files = [f for f in timeseries_files if f[0] in aggregate_timeseries_names]\n",
    "\n",
    "# Determine sets of remaining filenames and years.\n",
    "filtered_filenames = set([f[0] for f in timeseries_files])\n",
    "filtered_years = set([f[-1] for f in timeseries_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea58b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read .gdx data into a nested dictionary with `year`->`param_name` as the keys.\n",
    "\n",
    "gdx_df_dict = dict() # Initialize empty dict for collecting input data.\n",
    "if rerun_data:\n",
    "    for (year, filename) in product(filtered_years, filtered_filenames):\n",
    "        gdx = gt.Container(f\"{input_folder_path}{filename}_{year}.gdx\") # Read input data file.\n",
    "        gdx_df_dict.setdefault(year, dict()) # Initialize empty sub-dictionary for parameter values.\n",
    "        for (param_name, vals) in gdx.data.items():\n",
    "            if param_name.startswith('ts_'): # Only record actual timeseries parameters, we're not interested in the dimensions.\n",
    "                gdx_df_dict[year][param_name] = vals.records # Record values per year per param_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919d7cd",
   "metadata": {},
   "source": [
    "## 3. Format data for TSAM\n",
    "\n",
    "All the data needs to be in a single dataframe with timesteps as indices for TSAM,\n",
    "while all timeseries values need to be pivoted to columns.\n",
    "\n",
    ">**NOTE!**\n",
    ">Currently, all timeseries are used raw, as-is, without any weighting.\n",
    ">Better results might be achieved via some form of weighting, but I'm unsure how that should be implemented in TSAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a1ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings for TSAM data formatting\n",
    "# Don't change these unless you know what you're doing.\n",
    "\n",
    "index_column_name = 't' # Name of the time index column.\n",
    "value_column_name = 'value' # Name of the value column.\n",
    "forecast_filter = {'f00'} # Filter out forecasts besides these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca2ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format data for TSAM\n",
    "# This unfortunately seems to take ~1.5 min for the full dataset,\n",
    "# which is way longer than it takes to deduce the representative periods\n",
    "# (at least employing the `hierarchical` method)\n",
    "\n",
    "if rerun_data:\n",
    "    tsam_df_dict = dict() # Initialize empty dict for collecting all timeseries per year.\n",
    "    for (year, param_dict) in gdx_df_dict.items():\n",
    "        for (param_name, vals) in param_dict.items():\n",
    "            agg_cols = vals.columns.difference([index_column_name, value_column_name]) # Gather all other column names to be aggregated.\n",
    "            agg_col_name = '-'.join([param_name, *agg_cols]) # Name for the aggregated column including parameter name.\n",
    "            if vals.get('f') is not None: # Forecast filtering applied when needed.\n",
    "                vals = vals.loc[vals['f'].isin(forecast_filter)]\n",
    "            vals[agg_col_name] = param_name + '-' + vals[agg_cols].agg('-'.join, axis=1) # Create aggregate column value.\n",
    "            vals = vals[[index_column_name, value_column_name, agg_col_name]] # Omit old columns.\n",
    "            vals = vals.pivot( # Pivot timeseries dataframe for TSAM\n",
    "                index=index_column_name, columns=agg_col_name, values=value_column_name\n",
    "            )\n",
    "            if tsam_df_dict.get(year) is None:\n",
    "                tsam_df_dict[year] = vals # First dataframe becomes the starting point.\n",
    "            else:\n",
    "                tsam_df_dict[year] = tsam_df_dict[year].join(vals) # Rest are joined on index.\n",
    "    with open(\"tsam_df_dict.pkl\", \"wb\") as file: # Save data for future use.\n",
    "        pickle.dump(tsam_df_dict, file)\n",
    "else:\n",
    "    with open(\"tsam_df_dict.pkl\", \"rb\") as file: # Load previous saved data.\n",
    "        tsam_df_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13449e27",
   "metadata": {},
   "source": [
    "## 4. Run TSAM and process Backbone samples.\n",
    "\n",
    "The most important settings below are `noTypicalPeriods` and `hoursPerPeriod`,\n",
    "maybe followed by `extremePeriodMethod` and `clusterMethod`.\n",
    "The rest don't really need to be touched, unless you know what you're doing.\n",
    "\n",
    "The full description of the TSAM parameters is not reproduced here,\n",
    "please refer to their [documentation](https://tsam.readthedocs.io/en/latest/timeseriesaggregationDoc.html) (somewhat technical as it is).\n",
    "Some of the concepts are more intuitively explained by their\n",
    "[recent methodology paper](https://doi.org/10.1016/j.apenergy.2022.119029)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configure TSAM aggregation\n",
    "# (see https://tsam.readthedocs.io/en/latest/timeseriesaggregationDoc.html)\n",
    "\n",
    "# Main settings you might be interested in tweaking.\n",
    "noTypicalPeriods = 4 # Number of representative periods. (Periods for hypertuning)\n",
    "hoursPerPeriod = 24 # Hours per representative period. (Segments for hypertuning?)\n",
    "extremePeriodMethod = \"replace_cluster_center\" # Method to integrate extreme periods? Honestly, this doesn't seem to impact much?\n",
    "clusterMethod = \"hierarchical\" # Select clustering method. (`hierarchical` or `k_medoids` recommended)\n",
    "\n",
    "# Auxiliary settings you shoudldn't need to touch.\n",
    "resolution = 1 # Resolution of input data in hours, shouldn't need to be touched.\n",
    "rescaleClusterPeriods = False # Don't rescale periods, we don't use that data anyhow.\n",
    "segmentation = True # Segmentation to enable hypertuning, doesn't impact the clustering results.\n",
    "numericalTolerance = 1e-6 # Set numerical tolerance for TSAM so it stops complaining (doesn't seem to work for some reason)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ca8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TSAM time series aggregation\n",
    "# This seems amazingly fast, less than 5 seconds for all the data.\n",
    "\n",
    "# Suppress warnings from TSAM (because `numericalTolerance` doesn't seem to work for some reason.)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tsam_dict = dict() # Initialize dict to store TSAM aggregation per year.\n",
    "for (year, data) in tsam_df_dict.items():\n",
    "    aggregation = tsam.TimeSeriesAggregation( ## Define TSAM aggregation.\n",
    "        data,\n",
    "        noTypicalPeriods=noTypicalPeriods,\n",
    "        hoursPerPeriod=hoursPerPeriod,\n",
    "        clusterMethod=clusterMethod,\n",
    "        resolution=resolution,\n",
    "        rescaleClusterPeriods=rescaleClusterPeriods,\n",
    "        extremePeriodMethod=extremePeriodMethod,\n",
    "        segmentation=segmentation,\n",
    "        numericalTolerance=numericalTolerance,\n",
    "    )\n",
    "    aggregation.createTypicalPeriods() ## Run TSAM aggregation.\n",
    "    tsam_dict[year] = aggregation\n",
    "\n",
    "# Re-enable warnings\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optional hypertuning test\n",
    "# Demoes hypertuning functionality with NE-data.\n",
    "\n",
    "hypertuning_year = None # Set a year for hypertuning\n",
    "if hypertuning_year is not None:\n",
    "    # Suppress warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    # Data reduction factor calculated based on given aggregation settings\n",
    "    reduction_factor = (\n",
    "        noTypicalPeriods * hoursPerPeriod / len(tsam_df_dict[hypertuning_year])\n",
    "    )\n",
    "    # Run hypertuning\n",
    "    hyper = hype.HyperTunedAggregations(tsam_dict[year])\n",
    "    result = hyper.identifyOptimalSegmentPeriodCombination(reduction_factor)\n",
    "    print(result)\n",
    "    warnings.filterwarnings('default')\n",
    "\n",
    "# E.g. apparently for 1990-1991 and 2000, 44 periods of 15 hours (44x15 = 660)\n",
    "# should be used instead of 4 weeks (4*168 = 672).\n",
    "\n",
    "# See `ne-model-hypertuning.ipynb` for a more thorough analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b172b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process Backbone samples per year from the aggregation\n",
    "# The samples are processed based on the \"clusters\" in TSAM.\n",
    "# NOTE! There is no guarantee that TSAM samples occur in a sequence similar to Backbone samples!\n",
    "# However, based on very brief testing this seems to mostly be the case? \n",
    "\n",
    "sample_dict = dict() # Initialize dict to store Backbone samples per year.\n",
    "for (year, agg) in tsam_dict.items():\n",
    "    sample_dict.setdefault(year, dict()) # Initialize empty dict for each year.\n",
    "    # First timestep index of each sample, directly from cluster center indices.\n",
    "    samples = {\n",
    "        val * hoursPerPeriod + 1: ind\n",
    "        for (val, ind) in zip_longest(agg.clusterCenterIndices, agg.clusterPeriodIdx)\n",
    "    }\n",
    "    # Calculate sample weights based on the cluster occurrence number.\n",
    "    sample_weights = agg.clusterPeriodNoOccur\n",
    "    normalized_sample_weights = {ind: val / sum(sample_weights.values()) for (ind, val) in sample_weights.items()}\n",
    "    # Order samples by starting index and fetch the corresponding lengths !!!\n",
    "    samples = [\n",
    "        (\"s\" + f\"{i}\".zfill(3), val, sample_weights[ind], normalized_sample_weights[ind])\n",
    "        for i, (val, ind) in enumerate(sorted(samples.items()))\n",
    "    ]\n",
    "    # Figure out the order the samples occur during the year\n",
    "    # This is not used at the moment, see note below.\n",
    "    #sample_order = agg.clusterOrder # Order of samples representing the raw data (year).\n",
    "    \n",
    "    # Record sample settings per year.\n",
    "    sample_dict[year][\"number_of_samples\"] = noTypicalPeriods\n",
    "    sample_dict[year][\"representative_period_length_in_hours\"] = hoursPerPeriod\n",
    "    sample_dict[year][\"sample_start_and_weight\"] = samples\n",
    "\n",
    "# How do we actually want to represent the year?\n",
    "# If we use the periods where they actually occur (clusterCenterIndices),\n",
    "# the weights won't match the outcome desired by TSAM when fed to Backbone.\n",
    "# If we instead aim to match the weights, the periods cannot occur where Backbone needs them to.\n",
    "# Then again, we need to place the samples where they are to get the correct data\n",
    "# into Backbone (agg.clusterCenterIndices), so I guess we can't really consider the order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cdba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example output from sample_dict\n",
    "\n",
    "sample_dict[\"1990\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06e322",
   "metadata": {},
   "source": [
    "## 5. Output Backbone settings.\n",
    "\n",
    "Finally, we need to output the representative periods for Backbone.\n",
    "Ideally, this need to be done using the `weather_year` as the selector.\n",
    "I suppose the easiest (if not the cleanest) way is to create a separate file per year,\n",
    "similar to what is done with the timeseries.\n",
    "Essentially, we'll want to autocreate a `samples_<year>.inc` or similar,\n",
    "which can then be read in `investInit.gms` based on the given `climateYear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a754fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write `samples_<year>.inc` files containing autogenerated sample settings.\n",
    "# This is essentially autogenerated GAMS code...\n",
    "\n",
    "for (year, sample_data) in sample_dict.items():\n",
    "    with open(input_folder_path + f\"samples_{year}.inc\", 'w') as file:\n",
    "        # Frontmatter\n",
    "        file.write(\"* Automatically generated sample settings from `representative-period-processing.ipynb`\\n\")\n",
    "        file.write(f\"* {datetime.now().isoformat()}\\n\")\n",
    "        # Number of samples\n",
    "        file.write(\"\\n* Number of samples used by the model \\n\")\n",
    "        file.write(f\"mSettings('invest', 'samples') = {sample_data['number_of_samples']};\\n\")\n",
    "        # Clearing initial and central samples.\n",
    "        file.write(\"\\n* Clear initial and central samples \\n\")\n",
    "        file.write(\"ms_initial('invest', s)$(ord(s) <= mSettings('invest', 'samples')) = yes;\\n\")\n",
    "        file.write(\"ms_central('invest', s) = no;\\n\")\n",
    "        # Declare equal probabilities for all samples.\n",
    "        file.write(\"\\n* Equal probability for all samples \\n\")\n",
    "        file.write(\"p_msProbability(ms_initial) = 1;\\n\")\n",
    "        # Define sample properties\n",
    "        file.write(\"\\n* Define sample properties \\n\")\n",
    "        for (sample, start, weight, nweight) in sample_data['sample_start_and_weight']:\n",
    "            file.write(f\"* Sample {sample}\\n\")\n",
    "            file.write(f\"msStart('invest', '{sample}') = {start};\\n\")\n",
    "            file.write(f\"msEnd('invest', '{sample}') = {start + sample_data[\"representative_period_length_in_hours\"]};\\n\")\n",
    "            file.write(f\"p_msWeight('invest', '{sample}') = {weight};\\n\")\n",
    "            file.write(f\"p_msAnnuityWeight('invest', '{sample}') = {nweight};\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31fda28",
   "metadata": {},
   "source": [
    "### Using the autogenerated Backbone sample settings\n",
    "\n",
    "Essentially, what the above code does is automatically output Backbone sample settings.\n",
    "These are the settings typically given in e.g. `investInit.gms` or similar.\n",
    "The automatically generated `samples_<year>.inc` files look something like this:\n",
    "\n",
    "```gams\n",
    "* Automatically generated sample settings from `representative-period-processing.ipynb`\n",
    "* 2025-10-08T14:37:24.128610\n",
    "\n",
    "* Number of samples used by the model \n",
    "mSettings('invest', 'samples') = 12;\n",
    "\n",
    "* Clear initial and central samples \n",
    "ms_initial('invest', s)$(ord(s) <= mSettings('invest', 'samples')) = yes;\n",
    "ms_central('invest', s) = no;\n",
    "\n",
    "* Equal probability for all samples \n",
    "p_msProbability(ms_initial) = 1;\n",
    "\n",
    "* Define sample properties \n",
    "* Sample s000\n",
    "msStart('invest', 's000') = 169;\n",
    "msEnd('invest', 's000') = 193;\n",
    "p_msWeight('invest', 's000') = 16;\n",
    "p_msAnnuityWeight('invest', 's000') = 0.043835616438356165;\n",
    "* Sample s001\n",
    "msStart('invest', 's001') = 913;\n",
    "msEnd('invest', 's001') = 937;\n",
    "p_msWeight('invest', 's001') = 43;\n",
    "p_msAnnuityWeight('invest', 's001') = 0.1178082191780822;\n",
    "* Sample s002\n",
    "etc...\n",
    "```\n",
    "\n",
    "Currently, the autogeneration assumes all-initial no-central samples\n",
    "_(not completely sure what they mean)_, as well as an equal probability across all samples.\n",
    "You will need to tweak the above code or the output files manually\n",
    "in case you want to change these settings.\n",
    "The number of samples and their properties are based on the TSAM representatives.\n",
    "\n",
    ">**NOTE!**\n",
    ">The output samples are ordered based on their `msStart`!\n",
    ">Essentially, samples `s000`, `s001`, `s002`, etc. proceed sequentially\n",
    ">through the time steps, but will likely have \"gaps\" between each other.\n",
    "\n",
    "**In order to plug these into your model, you'll need to load them in your `investInit.gms`\n",
    "or similar.**\n",
    "_(See e.g. `temp_investInit.gms` in the Backbone repository)_\n",
    "Below an example for how to read the autogenerated sample settings into\n",
    "your model based on the given `%input_dir%` and `%climateYear%` command line\n",
    "arguments already used by the NE-model to select weather years from the data:\n",
    "\n",
    "```gams\n",
    "...\n",
    "* =============================================================================\n",
    "* --- Model Time Structure ----------------------------------------------------\n",
    "* =============================================================================\n",
    "\n",
    "* --- Define Samples ----------------------------------------------------------\n",
    "\n",
    "    $$include '%input_dir%\\samples_%climateYear%.inc' // THIS IS THE KEY ROW\n",
    "    // Currently, the NE-model risks infeasibility if a sample starts at 1.\n",
    "    // Temporary fix to delay by 1 hour.\n",
    "    msStart(ms_initial)${msStart(ms_initial) = 1} = 2;\n",
    "\n",
    "* --- Define Time Step Intervals ----------------------------------------------\n",
    "...\n",
    "```\n",
    "\n",
    "At the time of writing, the NE-model has a peculiar issue where hydropower reservoir\n",
    "initial states might cause an infeasibility IF they are released AND the first\n",
    "sample starts at the first available time step.\n",
    "The above code contains a manual bypass, where any sample starting on the first\n",
    "time step is forcibly delayed by one step to avoid the possible infeasibility.\n",
    "While it is quite rare for TSAM to begin a sample at the first available time step,\n",
    "it can still occur _(and did in fact occur in my testing)_.\n",
    "\n",
    "It should also be mentioned that depending on the type of modelling you're doing,\n",
    "you might have to adapt other parts of your Backbone inputs to accommodate\n",
    "the potentially varying properties of the samples.\n",
    "I.e. make sure the `timeAndSamples.inc` has a sufficient number of base sample indices.\n",
    "Additionally, defining `gnss_bound`s for how the samples bridge to themselves or each\n",
    "other might require tweaking.\n",
    "The `modelsInit.gms` in the `gg-smr-scenarios` branch of\n",
    "[my forked NE-model](https://github.com/Tasqu/north_european_model/tree/gg-smr-scenarios)\n",
    "contains potential examples for how these might be scripted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e062532d",
   "metadata": {},
   "source": [
    "# TODO?\n",
    "\n",
    "Interesting idea by Niina: First reduce the number of years by selecting \"representative years\", then represent them using \"representative periods\".\n",
    "This could significantly reduce the computational burden to represent multiple years.\n",
    "However, we would likely want to include different weather years for the investments, so I'm not sure if TSAM can handle that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
